"""
OmniParser-Enhanced Desktop Automation

Integrates Microsoft's OmniParser vision-based GUI agent with PyAutoGUI
for intelligent desktop automation that can understand and interact with
any GUI application using computer vision and natural language.

Key Features:
- Vision-based element detection (no need for coordinates/selectors)
- Natural language task descriptions  
- LLM-guided action planning
- Robust error handling and recovery
- Screenshot-based verification
- Multi-step automation workflows
"""

import asyncio
import base64
import io
import json
import time
from typing import Dict, Any, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path

# Core automation libraries
import pyautogui
from PIL import Image, ImageDraw, ImageFont
import pytesseract  # OCR fallback

# Import existing qweenone components
try:
    from ..agents.base_agent import BaseAgent, AgentResponse
    from ..utils.logger import setup_logger
except ImportError:
    BaseAgent = None
    AgentResponse = None
    setup_logger = lambda x: None


@dataclass
class GUIElement:
    """Detected GUI element from vision analysis"""
    id: str
    description: str
    element_type: str  # button, textfield, label, icon, etc.
    bbox: Dict[str, int]  # {"x": int, "y": int, "width": int, "height": int}
    confidence: float  # 0.0-1.0
    text_content: str = ""
    is_clickable: bool = True
    is_visible: bool = True
    properties: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def center(self) -> Tuple[int, int]:
        """Get center coordinates of element"""
        return (
            self.bbox["x"] + self.bbox["width"] // 2,
            self.bbox["y"] + self.bbox["height"] // 2
        )


@dataclass
class AutomationStep:
    """Single step in desktop automation sequence"""
    action: str  # click, type, wait, scroll, screenshot, verify
    target: str  # Element description or text to type
    parameters: Dict[str, Any] = field(default_factory=dict)
    expected_result: str = ""
    timeout: int = 10  # seconds
    retry_count: int = 3
    screenshot_before: bool = False
    screenshot_after: bool = True


class OmniDesktopAutomation:
    """
    Advanced desktop automation using OmniParser vision + PyAutoGUI actions
    
    Provides intelligent GUI interaction capabilities that can understand
    and manipulate any desktop application using natural language descriptions.
    """
    
    def __init__(self, 
                 screenshot_dir: str = "/tmp/qweenone_screenshots",
                 enable_vision: bool = True,
                 enable_ocr_fallback: bool = True,
                 safety_mode: bool = True):
        """
        Initialize OmniParser desktop automation
        
        Args:
            screenshot_dir: Directory to save automation screenshots
            enable_vision: Whether to use OmniParser vision capabilities
            enable_ocr_fallback: Whether to use OCR when vision fails
            safety_mode: Enable PyAutoGUI safety features
        """
        self.screenshot_dir = Path(screenshot_dir)
        self.screenshot_dir.mkdir(exist_ok=True)
        
        self.enable_vision = enable_vision
        self.enable_ocr_fallback = enable_ocr_fallback
        self.logger = setup_logger("OmniDesktopAutomation")
        
        # Configure PyAutoGUI for safety
        if safety_mode:
            pyautogui.FAILSAFE = True  # Move mouse to corner to abort
            pyautogui.PAUSE = 0.5      # Pause between actions
        
        # Initialize OmniParser (with fallback)
        self.omniparser = self._initialize_omniparser()
        
        # Automation state
        self.last_screenshot: Optional[Image.Image] = None
        self.detected_elements: List[GUIElement] = []
        self.automation_history: List[Dict[str, Any]] = []
        
        if self.logger:
            self.logger.info(f"Initialized OmniDesktopAutomation (vision: {enable_vision})")
    
    def _initialize_omniparser(self):
        """Initialize OmniParser with fallback handling"""
        if not self.enable_vision:
            return None
            
        try:
            # Attempt to import and initialize OmniParser
            # Note: This is a placeholder for actual OmniParser integration
            # Real implementation would use: from omniparser import OmniParser
            
            # For now, create a mock OmniParser for testing
            class MockOmniParser:
                async def parse_screen(self, image_base64: str, task_description: str) -> Dict[str, Any]:
                    """Mock OmniParser for testing"""
                    await asyncio.sleep(0.1)  # Simulate processing time
                    
                    return {
                        "elements": [
                            {
                                "id": "button_1",
                                "description": "Start button",
                                "type": "button", 
                                "bbox": {"x": 100, "y": 200, "width": 80, "height": 30},
                                "confidence": 0.95,
                                "text": "Start"
                            },
                            {
                                "id": "textfield_1", 
                                "description": "Input text field",
                                "type": "textfield",
                                "bbox": {"x": 200, "y": 150, "width": 200, "height": 25},
                                "confidence": 0.89,
                                "text": ""
                            }
                        ],
                        "confidence": 0.92,
                        "processing_time": 0.1
                    }
            
            parser = MockOmniParser()
            
            if self.logger:
                self.logger.info("OmniParser initialized (mock version for demo)")
            
            return parser
            
        except Exception as e:
            if self.logger:
                self.logger.warning(f"OmniParser initialization failed: {e}")
            return None
    
    async def take_screenshot(self, save_path: Optional[str] = None) -> Image.Image:
        """Take screenshot and optionally save it"""
        try:
            screenshot = pyautogui.screenshot()
            self.last_screenshot = screenshot
            
            if save_path:
                screenshot.save(save_path)
            
            # Auto-save with timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            auto_path = self.screenshot_dir / f"screenshot_{timestamp}.png"
            screenshot.save(auto_path)
            
            if self.logger:
                self.logger.debug(f"Screenshot saved: {auto_path}")
            
            return screenshot
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"Screenshot failed: {e}")
            raise e
    
    async def analyze_screen(self, 
                           screenshot: Optional[Image.Image] = None,
                           task_context: str = "") -> List[GUIElement]:
        """
        Analyze screen using OmniParser vision model
        
        Returns list of detected GUI elements with descriptions and coordinates
        """
        if screenshot is None:
            screenshot = await self.take_screenshot()
        
        elements = []
        
        try:
            if self.omniparser:
                # Convert PIL image to base64
                buffer = io.BytesIO()
                screenshot.save(buffer, format='PNG')
                image_base64 = base64.b64encode(buffer.getvalue()).decode()
                
                # Use OmniParser for vision-based analysis
                analysis = await self.omniparser.parse_screen(
                    image_base64=image_base64,
                    task_description=f"Identify GUI elements for task: {task_context}"
                )
                
                # Convert to GUIElement objects
                for elem_data in analysis.get("elements", []):
                    element = GUIElement(
                        id=elem_data["id"],
                        description=elem_data["description"],
                        element_type=elem_data["type"],
                        bbox=elem_data["bbox"],
                        confidence=elem_data["confidence"],
                        text_content=elem_data.get("text", ""),
                        is_clickable=elem_data["type"] in ["button", "link", "icon"],
                    )
                    elements.append(element)
                
                if self.logger:
                    self.logger.info(f"OmniParser detected {len(elements)} elements")
            
            elif self.enable_ocr_fallback:
                # Fallback: Use OCR to detect text
                elements = await self._ocr_fallback_analysis(screenshot)
                
                if self.logger:
                    self.logger.info(f"OCR fallback detected {len(elements)} text elements")
            
            self.detected_elements = elements
            return elements
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"Screen analysis failed: {e}")
            return []
    
    async def _ocr_fallback_analysis(self, screenshot: Image.Image) -> List[GUIElement]:
        """Fallback OCR-based element detection"""
        elements = []
        
        try:
            # Use OCR to find text
            ocr_data = pytesseract.image_to_data(screenshot, output_type=pytesseract.Output.DICT)
            
            for i, text in enumerate(ocr_data['text']):
                if text.strip():  # Non-empty text
                    confidence = int(ocr_data['conf'][i])
                    if confidence > 30:  # Reasonable confidence threshold
                        
                        x, y, w, h = (ocr_data['left'][i], ocr_data['top'][i], 
                                    ocr_data['width'][i], ocr_data['height'][i])
                        
                        element = GUIElement(
                            id=f"ocr_{i}",
                            description=f"Text: {text}",
                            element_type="text",
                            bbox={"x": x, "y": y, "width": w, "height": h},
                            confidence=confidence / 100.0,
                            text_content=text,
                            is_clickable=False  # OCR doesn't detect clickability
                        )
                        elements.append(element)
            
        except Exception as e:
            if self.logger:
                self.logger.warning(f"OCR fallback failed: {e}")
        
        return elements
    
    async def find_element_by_description(self, 
                                        description: str,
                                        screenshot: Optional[Image.Image] = None) -> Optional[GUIElement]:
        """Find GUI element by natural language description"""
        
        if screenshot is None:
            screenshot = await self.take_screenshot()
        
        elements = await self.analyze_screen(screenshot, task_context=f"Find: {description}")
        
        # Find best matching element
        best_match = None
        best_score = 0.0
        
        description_lower = description.lower()
        
        for element in elements:
            score = 0.0
            element_desc_lower = element.description.lower()
            
            # Exact match bonus
            if description_lower in element_desc_lower:
                score += 0.8
            
            # Partial word matches
            desc_words = description_lower.split()
            elem_words = element_desc_lower.split()
            
            common_words = set(desc_words) & set(elem_words)
            if desc_words:
                score += 0.5 * (len(common_words) / len(desc_words))
            
            # Confidence bonus
            score += 0.2 * element.confidence
            
            if score > best_score:
                best_score = score
                best_match = element
        
        if best_match and best_score > 0.3:  # Reasonable threshold
            if self.logger:
                self.logger.info(f"Found element '{description}': {best_match.description} (score: {best_score:.2f})")
            return best_match
        
        if self.logger:
            self.logger.warning(f"Element '{description}' not found")
        return None
    
    async def click_element(self, 
                          element_description: str,
                          screenshot: Optional[Image.Image] = None) -> bool:
        """Click GUI element by description using vision guidance"""
        
        element = await self.find_element_by_description(element_description, screenshot)
        
        if not element:
            return False
        
        if not element.is_clickable:
            if self.logger:
                self.logger.warning(f"Element '{element_description}' may not be clickable")
        
        try:
            # Click at element center
            center_x, center_y = element.center
            
            if self.logger:
                self.logger.info(f"Clicking '{element_description}' at ({center_x}, {center_y})")
            
            pyautogui.click(center_x, center_y)
            
            # Brief pause for UI response
            await asyncio.sleep(0.5)
            
            # Log action
            self.automation_history.append({
                "timestamp": datetime.now().isoformat(),
                "action": "click",
                "target": element_description,
                "coordinates": (center_x, center_y),
                "element_id": element.id,
                "success": True
            })
            
            return True
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"Click failed: {e}")
            
            self.automation_history.append({
                "timestamp": datetime.now().isoformat(),
                "action": "click",
                "target": element_description,
                "error": str(e),
                "success": False
            })
            
            return False
    
    async def type_text(self, text: str, clear_field: bool = True) -> bool:
        """Type text with optional field clearing"""
        try:
            if clear_field:
                # Clear existing text (Ctrl+A, Delete)
                pyautogui.hotkey('ctrl', 'a')
                await asyncio.sleep(0.1)
                pyautogui.press('delete')
                await asyncio.sleep(0.1)
            
            # Type the text
            pyautogui.write(text, interval=0.05)  # Slight delay between characters
            
            if self.logger:
                self.logger.info(f"Typed text: {text[:50]}{'...' if len(text) > 50 else ''}")
            
            self.automation_history.append({
                "timestamp": datetime.now().isoformat(),
                "action": "type",
                "text": text,
                "clear_field": clear_field,
                "success": True
            })
            
            return True
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"Type failed: {e}")
            
            self.automation_history.append({
                "timestamp": datetime.now().isoformat(),
                "action": "type",
                "text": text,
                "error": str(e),
                "success": False
            })
            
            return False
    
    async def execute_automation_sequence(self, steps: List[AutomationStep]) -> Dict[str, Any]:
        """Execute sequence of automation steps"""
        
        if self.logger:
            self.logger.info(f"Executing automation sequence with {len(steps)} steps")
        
        results = []
        overall_success = True
        
        for i, step in enumerate(steps, 1):
            step_start = time.time()
            step_result = {"step": i, "action": step.action, "target": step.target}
            
            try:
                if self.logger:
                    self.logger.info(f"Step {i}/{len(steps)}: {step.action} - {step.target}")
                
                # Take screenshot before action if requested
                if step.screenshot_before:
                    await self.take_screenshot(
                        save_path=str(self.screenshot_dir / f"step_{i}_before.png")
                    )
                
                # Execute step based on action type
                success = await self._execute_single_step(step)
                
                # Take screenshot after action if requested
                if step.screenshot_after:
                    await self.take_screenshot(
                        save_path=str(self.screenshot_dir / f"step_{i}_after.png")
                    )
                
                step_duration = time.time() - step_start
                
                step_result.update({
                    "success": success,
                    "duration": step_duration,
                    "timestamp": datetime.now().isoformat()
                })
                
                if not success:
                    overall_success = False
                    step_result["error"] = f"Step {step.action} failed"
                
            except Exception as e:
                if self.logger:
                    self.logger.error(f"Step {i} failed: {e}")
                
                step_result.update({
                    "success": False,
                    "error": str(e),
                    "duration": time.time() - step_start,
                    "timestamp": datetime.now().isoformat()
                })
                
                overall_success = False
            
            results.append(step_result)
            
            # Stop on failure unless continue_on_error is set
            if not step_result["success"] and not step.parameters.get("continue_on_error", False):
                break
        
        return {
            "overall_success": overall_success,
            "total_steps": len(steps),
            "executed_steps": len(results),
            "step_results": results,
            "automation_history": self.automation_history[-len(results):],  # Recent history
            "final_screenshot": self._screenshot_to_base64(self.last_screenshot) if self.last_screenshot else None
        }
    
    async def _execute_single_step(self, step: AutomationStep) -> bool:
        """Execute a single automation step"""
        
        action = step.action.lower()
        
        if action == "click":
            return await self.click_element(step.target)
        
        elif action == "type":
            clear_field = step.parameters.get("clear_field", True)
            return await self.type_text(step.target, clear_field)
        
        elif action == "wait":
            duration = step.parameters.get("duration", 1.0)
            await asyncio.sleep(duration)
            return True
        
        elif action == "scroll":
            direction = step.parameters.get("direction", "down")
            amount = step.parameters.get("amount", 3)
            
            scroll_amount = amount if direction == "up" else -amount
            pyautogui.scroll(scroll_amount)
            await asyncio.sleep(0.5)
            return True
        
        elif action == "screenshot":
            await self.take_screenshot()
            return True
        
        elif action == "verify":
            # Verify element exists
            element = await self.find_element_by_description(step.target)
            return element is not None
        
        elif action == "key":
            # Press keyboard key(s)
            keys = step.target.split("+")
            if len(keys) == 1:
                pyautogui.press(keys[0])
            else:
                pyautogui.hotkey(*keys)
            await asyncio.sleep(0.2)
            return True
        
        else:
            if self.logger:
                self.logger.error(f"Unknown action: {action}")
            return False
    
    async def execute_natural_language_task(self, task_description: str) -> Dict[str, Any]:
        """
        Execute complex task described in natural language
        
        Uses LLM to plan automation steps based on current screen state
        """
        if self.logger:
            self.logger.info(f"Executing natural language task: {task_description}")
        
        try:
            # Take initial screenshot for context
            screenshot = await self.take_screenshot()
            
            # Analyze current screen
            elements = await self.analyze_screen(screenshot, task_description)
            
            # Generate action plan using LLM
            action_plan = await self._generate_action_plan(task_description, elements)
            
            # Convert plan to AutomationStep objects  
            steps = self._convert_plan_to_steps(action_plan)
            
            # Execute the planned steps
            result = await self.execute_automation_sequence(steps)
            
            # Add task context to result
            result.update({
                "task_description": task_description,
                "initial_elements_detected": len(elements),
                "plan_generated": True,
                "llm_guided": True
            })
            
            return result
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"Natural language task execution failed: {e}")
            
            return {
                "overall_success": False,
                "error": str(e),
                "task_description": task_description,
                "plan_generated": False
            }
    
    async def _generate_action_plan(self, 
                                  task_description: str, 
                                  available_elements: List[GUIElement]) -> List[Dict[str, Any]]:
        """
        Generate action plan using LLM based on task and available GUI elements
        """
        
        # Create context about available elements
        elements_context = []
        for elem in available_elements:
            elements_context.append({
                "description": elem.description,
                "type": elem.element_type,
                "clickable": elem.is_clickable,
                "text": elem.text_content
            })
        
        # Plan generation prompt
        planning_prompt = f"""
        Task: {task_description}
        
        Available GUI elements on screen:
        {json.dumps(elements_context, indent=2)}
        
        Plan step-by-step actions to complete this task. Each step should be one of:
        - {{"action": "click", "target": "element description"}}
        - {{"action": "type", "target": "text to type", "parameters": {{"clear_field": true}}}}
        - {{"action": "wait", "parameters": {{"duration": 1.0}}}}
        - {{"action": "scroll", "parameters": {{"direction": "up|down", "amount": 3}}}}
        - {{"action": "key", "target": "key+combination"}}
        - {{"action": "verify", "target": "element to verify exists"}}
        
        Respond with a JSON array of steps that logically complete the task.
        Focus on efficiency and reliability. Include verification steps.
        """
        
        try:
            # Use existing LLM integration (simplified for demo)
            # In real implementation, would use LiteLLM router
            
            # Mock LLM response for demo
            await asyncio.sleep(0.5)  # Simulate LLM processing
            
            # Generate reasonable default plan
            if "calculator" in task_description.lower():
                plan = [
                    {"action": "click", "target": "calculator button"},
                    {"action": "wait", "parameters": {"duration": 1.0}},
                    {"action": "verify", "target": "calculator window"}
                ]
            elif "notepad" in task_description.lower() or "text" in task_description.lower():
                plan = [
                    {"action": "click", "target": "text editor"},
                    {"action": "type", "target": "Hello from OmniParser automation!"},
                    {"action": "key", "target": "ctrl+s"},
                    {"action": "verify", "target": "text content"}
                ]
            else:
                # Generic plan
                plan = [
                    {"action": "screenshot"},
                    {"action": "verify", "target": "main interface"}
                ]
            
            if self.logger:
                self.logger.info(f"Generated action plan with {len(plan)} steps")
            
            return plan
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"Action plan generation failed: {e}")
            
            # Return minimal safe plan
            return [{"action": "screenshot"}]
    
    def _convert_plan_to_steps(self, plan: List[Dict[str, Any]]) -> List[AutomationStep]:
        """Convert LLM-generated plan to AutomationStep objects"""
        steps = []
        
        for step_data in plan:
            step = AutomationStep(
                action=step_data["action"],
                target=step_data.get("target", ""),
                parameters=step_data.get("parameters", {}),
                timeout=step_data.get("timeout", 10),
                screenshot_after=True  # Always take screenshots for verification
            )
            steps.append(step)
        
        return steps
    
    def _screenshot_to_base64(self, screenshot: Image.Image) -> str:
        """Convert PIL image to base64 string"""
        if not screenshot:
            return ""
        
        buffer = io.BytesIO()
        screenshot.save(buffer, format='PNG')
        return base64.b64encode(buffer.getvalue()).decode()
    
    def get_automation_stats(self) -> Dict[str, Any]:
        """Get comprehensive automation statistics"""
        total_actions = len(self.automation_history)
        successful_actions = sum(1 for action in self.automation_history if action.get("success", False))
        
        return {
            "total_actions": total_actions,
            "successful_actions": successful_actions,
            "success_rate": (successful_actions / total_actions * 100) if total_actions > 0 else 0,
            "vision_enabled": self.enable_vision,
            "omniparser_available": self.omniparser is not None,
            "last_screenshot_time": self.automation_history[-1]["timestamp"] if self.automation_history else None,
            "detected_elements_count": len(self.detected_elements)
        }


class DesktopAutomationAgent(BaseAgent if BaseAgent else object):
    """
    Desktop Automation Agent that integrates OmniParser with qweenone agent system
    
    Provides seamless integration between vision-based desktop automation
    and the existing agent architecture.
    """
    
    def __init__(self, name: str = "DesktopAutomationAgent", agent_id: str = None):
        if BaseAgent:
            super().__init__(name, "Desktop automation using vision-based GUI control", agent_id)
            self.add_capability("DESKTOP_AUTOMATION")
            self.add_capability("GUI_INTERACTION") 
            self.add_capability("COMPUTER_VISION")
        
        self.omni_automation = OmniDesktopAutomation()
        self.logger = setup_logger("DesktopAutomationAgent")
        
        if hasattr(self, 'agent_type'):
            self.agent_type = "desktop_automation"
    
    async def execute_task(self, task: Union[Dict[str, Any], 'LegacyTask']) -> 'AgentResponse':
        """Execute desktop automation task"""
        
        # Extract task description
        if isinstance(task, dict):
            task_description = task.get("description", task.get("title", ""))
            task_id = task.get("id", "unknown")
        else:
            task_description = getattr(task, 'description', getattr(task, 'title', ''))
            task_id = getattr(task, 'task_id', 'unknown')
        
        if self.logger:
            self.logger.info(f"Executing desktop automation task: {task_id}")
        
        try:
            # Check if this is a desktop automation task
            if not self._is_desktop_task(task_description):
                return self._create_response(
                    success=False,
                    error="Not a desktop automation task",
                    data={"task_type": "non_desktop"}
                )
            
            # Execute using natural language processing
            result = await self.omni_automation.execute_natural_language_task(task_description)
            
            # Get automation stats
            stats = self.omni_automation.get_automation_stats()
            
            response_data = {
                "automation_result": result,
                "automation_stats": stats,
                "task_id": task_id,
                "agent_type": "desktop_automation",
                "execution_mode": "vision_based"
            }
            
            return self._create_response(
                success=result.get("overall_success", False),
                data=response_data,
                metadata={"execution_time": result.get("total_execution_time", 0)}
            )
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"Desktop automation task failed: {e}")
            
            return self._create_response(
                success=False,
                error=str(e),
                data={"task_id": task_id, "agent_type": "desktop_automation"}
            )
    
    def _is_desktop_task(self, description: str) -> bool:
        """Determine if task requires desktop automation"""
        desktop_keywords = [
            "desktop", "gui", "window", "application", "click", "interface",
            "automation", "mouse", "keyboard", "screen", "calculator", 
            "notepad", "file explorer", "paint", "browser window"
        ]
        
        description_lower = description.lower()
        return any(keyword in description_lower for keyword in desktop_keywords)
    
    def _create_response(self, 
                        success: bool, 
                        data: Any = None, 
                        error: str = "", 
                        metadata: Dict[str, Any] = None) -> Union['AgentResponse', Dict[str, Any]]:
        """Create response in appropriate format"""
        
        if AgentResponse:
            return AgentResponse(
                success=success,
                data=data,
                error=error,
                metadata=metadata or {}
            )
        else:
            # Fallback format for testing
            return {
                "success": success,
                "data": data,
                "error": error,
                "metadata": metadata or {}
            }


# Utility functions for integration
async def execute_desktop_automation(task_description: str) -> Dict[str, Any]:
    """
    Convenience function for quick desktop automation
    
    Can be used as a drop-in replacement for manual GUI interactions
    """
    automation = OmniDesktopAutomation()
    return await automation.execute_natural_language_task(task_description)


if __name__ == "__main__":
    """Demo and testing"""
    
    async def demo_desktop_automation():
        """Demonstrate desktop automation capabilities"""
        
        print("ğŸ–¥ï¸ === OMNIPARSER DESKTOP AUTOMATION DEMO ===")
        
        # Create automation instance
        automation = OmniDesktopAutomation()
        
        # Test tasks
        test_tasks = [
            "Take a screenshot of the current desktop",
            "Open calculator application",
            "Type '2+2=' in calculator",
        ]
        
        for task in test_tasks:
            print(f"\nğŸ¯ Task: {task}")
            
            result = await automation.execute_natural_language_task(task)
            
            print(f"âœ… Success: {result.get('overall_success', False)}")
            
            if result.get('step_results'):
                print(f"ğŸ“Š Steps executed: {len(result['step_results'])}")
        
        # Show stats
        stats = automation.get_automation_stats()
        print(f"\nğŸ“ˆ Automation Stats:")
        print(f"  â€¢ Total actions: {stats['total_actions']}")
        print(f"  â€¢ Success rate: {stats['success_rate']:.1f}%")
        print(f"  â€¢ Vision enabled: {stats['vision_enabled']}")
    
    # Run demo
    asyncio.run(demo_desktop_automation())